<!-- Greeting -->
<h2><em> Hi, I'm Wenhao Wu :wave: </em></h2>

<p> 
<a href="https://whwu95.github.io"><img src="https://img.shields.io/badge/Wenhao%20Wu-Homepage-red?style=flat-square" height="25px" alt="Wenhao Wu"></a>
<a href="https://www.zhihu.com/people/wu-wen-hao-80-23"><img src="https://img.shields.io/badge/Áü•‰πé-0079FF.svg?style=flat-square&logo=zhihu&logoColor=white" height="25px" alt="Áü•‰πé"></a>
<a href="https://www.linkedin.com/in/wenhao-w-usyd/"><img src="https://img.shields.io/badge/linkedin-006CAC.svg?&style=flat-square&logo=linkedin&logoColor=white" height="25px" alt="LinkedIn"></a>
<a href="https://scholar.google.com/citations?user=Kn5d1ckAAAAJ&hl=en"><img src="https://img.shields.io/badge/Google%20Scholar-4285F4?style=flat-square&logo=google-scholar&logoColor=white" height="25px" alt="Google Scholar"></a>
<a href="https://twitter.com/DrWenhaoWu"><img src="https://img.shields.io/badge/X-%23000000.svg?style=flat-square&logo=X&logoColor=white" height="25px" alt="X"></a>
</p> 

**Wenhao Wu (Âê¥ÊñáÁÅèüá®üá≥)** is a Second-Year Ph.D. student in the School of Computer Science at [The University of Sydney](https://www.sydney.edu.au/), supervised by [Prof. Wanli Ouyang](https://wlouyang.github.io/). I have a close collaboration with Department of Computer Vision Technology ([VIS](https://vis.baidu.com/)) at Baidu led by [Dr. Jingdong Wang (IEEE Fellow)](https://jingdongwang2017.github.io/). I received my M.S.E degree from Multimedia Laboratory ([MMLab@SIAT](http://mmlab.siat.ac.cn/)), [University of Chinese Academy of Sciences](http://english.ucas.ac.cn/), supervised by [Prof. Shifeng Chen](https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en) and [Prof. Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/).
I was also fortunate to intern/RA at MMLab@CUHK, Baidu, iQIYI, SenseTime, Samsung Research and Chinese Academy of Sciences.

My current research interest includes **Cross-Modal Learning** and **Video Understanding**. I have published **20+** papers at the top international CV/AI conferences or journals such as **CVPR/ICCV/ECCV/AAAI/IJCAI/ACMMM/IJCV**.

**I am actively looking for research internship position (US/AU/Remote) starting from 2024 Spring/Summer. Additionally, I am open to academic collaborations. Please feel free to drop me an email.**

<a href="https://github.com/whwu95"><img align="center" src="https://github-readme-stats.vercel.app/api?username=whwu95&show_icons=true&theme=ambient_gradient&hide=contribs,prs&count_private=true&include_all_commits=true&show_owner=ture" alt="whwu95's github stats" /></a> 




### üî• News
- *2023.11*: We release [GPT4Vis](https://arxiv.org/abs/2311.15732)  [![](https://img.shields.io/github/stars/whwu95/GPT4Vis?color=success&logo=github)](https://github.com/whwu95/GPT4Vis), which provides a Quantitative Evaluation of GPT-4 for Visual Understanding across images, videos and point clouds, spinning on 16 popular datasets.
- *2023.11*: We release [Side4Video](https://arxiv.org/abs/2311.15769) [![](https://img.shields.io/github/stars/HJYao00/Side4Video?color=success&logo=github)](https://github.com/HJYao00/Side4Video), a Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning, which significantly reduces the training memory cost for action recognition (‚Üì75%) and text-video retrieval (‚Üì30%).
- *2023.08*: [The extension of Text4Vis](https://link.springer.com/article/10.1007/s11263-023-01876-w) [![](https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github)](https://github.com/whwu95/Text4Vis) has been accepted by International Journal of Computer Vision (IJCV).
- *2023.07*: Two First-author papers (Temporal Modeling: [ATM](https://github.com/whwu95/ATM) [![](https://img.shields.io/github/stars/whwu95/ATM?color=success&logo=github)](https://github.com/whwu95/ATM), Cross-Modal Retrieval: [UA](https://arxiv.org/abs/2301.06309) are accepted by ICCV2023!
- *2023.02*: Two First-author papers for video understanding ([BIKE](https://github.com/whwu95/BIKE) [![](https://img.shields.io/github/stars/whwu95/BIKE?color=success&logo=github)](https://github.com/whwu95/BIKE), [Cap4Video](https://github.com/whwu95/Cap4Video) [![](https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github)](https://github.com/whwu95/Cap4Video)) are accepted by CVPR 2023. Cap4Video involves GPT to enhance text-video learning, is selected as a üéâ**Highlight paper**üéâ (Top 2.5%).
- *2022.11*: Two papers (Video Recognition: [Text4Vis](https://github.com/whwu95/Text4Vis) [![](https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github)](https://github.com/whwu95/Text4Vis), Style Transfer: AdaCM) are accepted by AAAI 2023.
- *2022.07*: Three papers (Video Sampling: [NSNet](https://arxiv.org/pdf/2207.10388.pdf), [TSQNet](https://arxiv.org/pdf/2207.10379.pdf), Cross-Modal Learning: [CODER](https://arxiv.org/pdf/2208.09843.pdf)) are accepted by ECCV 2022.
- *2022.06*: Our [MaMiCo](https://dl.acm.org/doi/10.1145/3503161.3547888), a new video self-supervised learning work, is accepted by ACMMM 2022 (üéâ**Oral Presentation**üéâ).

