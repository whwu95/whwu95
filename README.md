## Hi there üëã

**Wenhao Wu (Âê¥ÊñáÁÅèüá®üá≥)** is a Second-Year Ph.D. student in the School of Computer Science at [The University of Sydney](https://www.sydney.edu.au/), supervised by [Prof. Wanli Ouyang](https://wlouyang.github.io/). I have a close collaboration with Department of Computer Vision Technology ([VIS](https://vis.baidu.com/)) at Baidu led by [Dr. Jingdong Wang (IEEE Fellow)](https://jingdongwang2017.github.io/). I received my M.S.E degree from Multimedia Laboratory ([MMLab@SIAT](http://mmlab.siat.ac.cn/)), [University of Chinese Academy of Sciences](http://english.ucas.ac.cn/), supervised by [Prof. Shifeng Chen](https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en) and [Prof. Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/).
I was also fortunate to intern/RA at MMLab@CUHK, Baidu, iQIYI, SenseTime, Samsung Research and Chinese Academy of Sciences.

My current research interest includes Cross-Modal Learning and Video Understanding. I have published **20+** papers at the top international CV/AI conferences or journals such as **CVPR/ICCV/ECCV/AAAI/IJCAI/ACMMM/IJCV**.

**I am actively looking for research internship position (US/AU/Remote) starting from 2024 Spring/Summer, feel free to drop me an email.**

### üìé Homepages
- Personal Pages: https://whwu95.github.io/ (Updated recentlyüî•)
- Linkedin: https://www.linkedin.com/in/wenhao-w-usyd/
- Google Scholar: https://scholar.google.com/citations?user=Kn5d1ckAAAAJ&hl=en

### üî• News
- *2023.11*: We release [GPT4Vis](https://arxiv.org/abs/2311.15732)  [![](https://img.shields.io/github/stars/whwu95/GPT4Vis?color=success&logo=github)](https://github.com/whwu95/GPT4Vis), which provides a Quantitative Evaluation of GPT-4 for Visual Understanding across images, videos and point clouds, spinning on 16 popular datasets.
- *2023.11*: We release [Side4Video](https://arxiv.org/abs/2311.15769) [![](https://img.shields.io/github/stars/HJYao00/Side4Video?color=success&logo=github)](https://github.com/HJYao00/Side4Video), a Spatial-Temporal Side Network for Memory-Efficient Image-to-Video Transfer Learning, which significantly reduces the training memory cost for action recognition (‚Üì75%) and text-video retrieval (‚Üì30%).
- *2023.08*: [The extension of Text4Vis](https://link.springer.com/article/10.1007/s11263-023-01876-w) [![](https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github)](https://github.com/whwu95/Text4Vis) has been accepted by International Journal of Computer Vision (IJCV).
- *2023.07*: Two First-author papers (Temporal Modeling: [ATM](https://github.com/whwu95/ATM) [![](https://img.shields.io/github/stars/whwu95/ATM?color=success&logo=github)](https://github.com/whwu95/ATM), Cross-Modal Retrieval: [UA](https://arxiv.org/abs/2301.06309) are accepted by ICCV2023!
- *2023.02*: Two First-author papers for video understanding ([BIKE](https://github.com/whwu95/BIKE) [![](https://img.shields.io/github/stars/whwu95/BIKE?color=success&logo=github)](https://github.com/whwu95/BIKE), [Cap4Video](https://github.com/whwu95/Cap4Video) [![](https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github)](https://github.com/whwu95/Cap4Video)) are accepted by CVPR 2023. Cap4Video involves GPT to enhance text-video learning, is selected as a üéâ**Highlight paper**üéâ (Top 2.5%).
- *2022.11*: Two papers (Video Recognition: [Text4Vis](https://github.com/whwu95/Text4Vis) [![](https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github)](https://github.com/whwu95/Text4Vis), Style Transfer: AdaCM) are accepted by AAAI 2023.
- *2022.07*: Three papers (Video Sampling: [NSNet](https://arxiv.org/pdf/2207.10388.pdf), [TSQNet](https://arxiv.org/pdf/2207.10379.pdf), Cross-Modal Learning: [CODER](https://arxiv.org/pdf/2208.09843.pdf)) are accepted by ECCV 2022.
- *2022.06*: Our [MaMiCo](https://dl.acm.org/doi/10.1145/3503161.3547888), a new video self-supervised learning work, is accepted by ACMMM 2022 (üéâ**Oral Presentation**üéâ).

