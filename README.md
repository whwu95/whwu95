<!-- Greeting -->
<h2><em> Hi, I'm Wenhao Wu :wave: </em></h2>

<p> 
<a href="https://whwu95.github.io"><img src="https://img.shields.io/badge/Wenhao%20Wu-Homepage-red?style=flat-square" height="25px" alt="Wenhao Wu"></a>
<a href="https://www.zhihu.com/people/wu-wen-hao-80-23"><img src="https://img.shields.io/badge/Áü•‰πé-0079FF.svg?style=flat-square&logo=zhihu&logoColor=white" height="25px" alt="Áü•‰πé"></a>
<a href="https://github.com/whwu95"><img src="https://img.shields.io/badge/github-%23121011.svg?style=flat-square&logo=github&logoColor=white" height="25px" alt="github"></a>
<a href="https://www.linkedin.com/in/wenhao-w-usyd/"><img src="https://img.shields.io/badge/linkedin-006CAC.svg?&style=flat-square&logo=linkedin&logoColor=white" height="25px" alt="LinkedIn"></a>
<a href="https://scholar.google.com/citations?user=Kn5d1ckAAAAJ&hl=en"><img src="https://img.shields.io/badge/Google%20Scholar-4285F4?style=flat-square&logo=google-scholar&logoColor=white" height="25px" alt="Google Scholar"></a>
<a href="https://twitter.com/DrWenhaoWu"><img src="https://img.shields.io/badge/X-%23000000.svg?style=flat-square&logo=X&logoColor=white" height="25px" alt="X"></a>
</p> 

**Wenhao Wu (Âê¥ÊñáÁÅèüá®üá≥)** is a Ph.D. student in the School of Computer Science at [The University of Sydney](https://www.sydney.edu.au/), supervised by [Prof. Wanli Ouyang](https://wlouyang.github.io/). I have a close collaboration with Department of Computer Vision Technology ([VIS](https://vis.baidu.com/)) at Baidu led by [Dr. Jingdong Wang (IEEE Fellow)](https://jingdongwang2017.github.io/). I received my M.S.E degree from Multimedia Laboratory ([MMLab@SIAT](http://mmlab.siat.ac.cn/)), [University of Chinese Academy of Sciences](http://english.ucas.ac.cn/), supervised by [Prof. Shifeng Chen](https://scholar.google.com/citations?user=6X77S3cAAAAJ&hl=en) and [Prof. Yu Qiao](http://mmlab.siat.ac.cn/yuqiao/).
I was also fortunate to intern/RA at MMLab@CUHK, Baidu, iQIYI, SenseTime, Samsung Research and Chinese Academy of Sciences.
I am honored to be awarded the 11th <a href="http://scholarship.baidu.com/home/index/index#banner"><b>Baidu PhD Fellowship</b></a> (2023).


My current research interest includes **Cross-Modal Learning** and **Video Understanding**. I have published **20+** papers at the top international CV/AI conferences or journals such as **CVPR/ICCV/ECCV/AAAI/IJCAI/ACMMM/TPAMI/IJCV**.

 <!-- **I am actively looking for research internship position (US/AU/Remote) starting from 2024 Winter. Additionally, I am open to academic collaborations. Please feel free to drop me an email.** --> 


![Wenhao Wu's GitHub stats](https://github-readme-stats.vercel.app/api?username=whwu95&show_icons=true&theme=default&hide=contribs,prs&count_private=true&include_all_commits=true&show_owner=ture)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=whwu95&layout=compact)

### üî≠ Research Interest

My research interests broadly lie in the areas of <b>Computer Vision</b> and <b>Deep Learning</b>, including:
- <strong style="font-size:15px;color:#8aa371">Cross-Modal Learning</strong> (2022-Present): Video-Language Matching, Multimodal Large Language Model (MLLM)
- <strong style="font-size:15px;color:#8aa371">Video Foundation Model</strong> (2017-Present): Video Recognition, Efficient Video Tuning
- Video-related Applications (2017-2022): Video Sampler, Temporal Action Detection, Anomaly Detction in Video
- Self-supervised Learning (2021-2022): Contrastive Video Learning, Masked Video Modeling
- Low-level Vision (2021-2022): Image Colorization, Style Transfer, Image Rescaling



### üî• News
- *2024.05*: <em> The extension of <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> <a href="https://github.com/whwu95/Cap4Video"><img src="https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github"></a> has been accepted by <strong>TPAMI</strong>.</em>
- *2024.01*: <em> I am honored to receive the 11thüéñ<a href="http://scholarship.baidu.com/home/index/index#banner"><font color="Red"><b>Baidu Scholarship</b></font></a>üéñ, a prestigious fellowship awarding <strong>200,000 RMB (about $30,000)</strong> to a select <strong>10 PhD students worldwide</strong> in Artificial Intelligence, selected from thousands of applicants.</em>
- *2023.11*: <em> We release <a href="https://arxiv.org/abs/2311.15732">GPT4Vis</a> <a href="https://github.com/whwu95/GPT4Vis"><img src="https://img.shields.io/github/stars/whwu95/GPT4Vis?color=success&logo=github"></a>, which provides a <strong>Quantitative Evaluation</strong> of GPT-4 for Visual Understanding across images, videos and point clouds, spinning on 16 popular datasets.</em>
- *2023.11*: <em> We release <a href="https://arxiv.org/abs/2311.15769">Side4Video</a> <a href="https://github.com/HJYao00/Side4Video"><img src="https://img.shields.io/github/stars/HJYao00/Side4Video?color=success&logo=github"></a>, a Spatial-Temporal Side Network for <strong>Memory-Efficient</strong> Image-to-Video Transfer Learning, which significantly reduces the training memory cost for action recognition (‚Üì75%) and text-video retrieval (‚Üì30%).</em>
- *2023.08*: <em> The extension of <a href="https://github.com/whwu95/Text4Vis">Text4Vis</a> <a href="https://github.com/whwu95/Text4Vis"><img src="https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github"></a> has been accepted by <strong>IJCV</strong>.</em>
- *2023.07*: <em> <strong>Two</strong> First-author papers (Temporal Modeling: <a href="https://github.com/whwu95/ATM">ATM</a> <a href="https://github.com/whwu95/ATM"><img src="https://img.shields.io/github/stars/whwu95/ATM?color=success&logo=github"></a>, Cross-Modal Retrieval: <a href="https://arxiv.org/abs/2301.06309">UA</a> <a href="https://github.com/bofang98/UATVR"><img src="https://img.shields.io/github/stars/bofang98/UATVR?color=success&logo=github"></a>) are accepted by <strong>ICCV2023</strong>.</em>
- *2023.02*: <em> <strong>Two</strong> First-author papers for video understanding (<a href="https://github.com/whwu95/BIKE">BIKE</a> <a href="https://github.com/whwu95/BIKE"><img src="https://img.shields.io/github/stars/whwu95/BIKE?color=success&logo=github"></a>, <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> <a href="https://github.com/whwu95/Cap4Video"><img src="https://img.shields.io/github/stars/whwu95/Cap4Video?color=success&logo=github"></a>) are accepted by <strong>CVPR 2023</strong>. <a href="https://github.com/whwu95/Cap4Video">Cap4Video</a> involves GPT to enhance text-video learning, is selected as a üéâ**Highlight paper**üéâ (<strong>Top 2.5%</strong>).</em>
- *2022.11*: <em> <strong>Two</strong> papers (Video Recognition: <a href="https://arxiv.org/pdf/2207.01297.pdf">Text4Vis</a> <a href="https://github.com/whwu95/Text4Vis"><img src="https://img.shields.io/github/stars/whwu95/Text4Vis?color=success&logo=github"></a>, Style Transfer: <a href="https://arxiv.org/pdf/2212.01567.pdf">AdaCM</a>) are accepted by <strong>AAAI 2023</strong>.</em>
- *2022.07*: <em> <strong>Three</strong> papers (Video Sampling: <a href="https://arxiv.org/pdf/2207.10388.pdf">NSNet</a>, <a href="https://arxiv.org/pdf/2207.10379.pdf">TSQNet</a>, Cross-Modal Learning: <a href="https://arxiv.org/pdf/2208.09843.pdf">CODER</a>) are accepted by <strong>ECCV 2022</strong>.</em>
- *2022.06*: <em> Our <a href="https://dl.acm.org/doi/10.1145/3503161.3547888">MaMiCo</a>, a new video self-supervised learning work, is accepted by <strong>ACMMM 2022</strong> (üéâ**Oral Presentation**üéâ).</em>
